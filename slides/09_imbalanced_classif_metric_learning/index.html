<!DOCTYPE html>
<html>
  <head>
    <title>Deep Learning Lectures</title>
    <meta charset="utf-8">
    <style>
     .left-column {
       width: 50%;
       float: left;
     }
     .right-column {
       width: 50%;
       float: right;
     }
     .grey { color: #bbbbbb; }
      </style>
    <link rel="stylesheet" type="text/css" href="slides.css">
  </head>
  <body>
      <textarea id="source">
class: center, middle

# Class imbalance and Metric Learning

Charles Ollion - Olivier Grisel

.affiliations[
  ![Heuritech](images/logo heuritech v2.png)
  ![Inria](images/inria-logo.png)
  ![UPS](images/Logo_Master_Datascience.png)
]

---
## Classic Deep Learning data setup

- Classification with a single label per sample
- 2-1000 classes ; 1000 samples per class

--

### real life problems

What if I have multiple classes  per sample?

--

What if I have unbalanced or noisy labels?

--

what if I have 1M classes and 10 samples / class?

---
## Outline

### Multi-labeling and Sampling strategies

--

### Metric Learning and siamese networks

--

### Dealing with weak and noisy labels

---
class: middle, center

# Multi-labeling and Sampling strategies

---

## From Classification to multilabel


_already seen in Object detection_ 

### todo

Train in classification scheme
Replace `softmax` with `sigmoid` activation function


training with binary labels

---
class: middle, center

# Metric Learning & Siamese networks

---
## Goal: learn systems with few examples

ex: Face Recognition/Verification

--

- Recognition: Given a face, classify among K possible persons
- Verification: verify that two faces belongs to the same person

todo add image face

--

A verification system = similarity measure. If it's really good, useful for recognition
---
## Learning a distance function


We want to build $d_{\theta}( x_1, x_2 )$ which indicates degree of difference between images

--

we define $T$ such that if $d_{\theta}( x_1, x_2 ) < T$, we consider $x_1$ and $x_2$ to be of the same class (e.g. identity)

--

**Not a real distance** (no guarantee for triangle inequality)

If you want guarrantees, look towards Metric Learning
(but works with linear projections, much less useful for images)

--



---
## Learning a distance function

$$
d\_{\theta}( x\_1, x\_2 ) = || f\_{\theta}(x\_1) - f\_{\theta}(x\_2) ||\_2
$$

--

We build representation space of the data in which a simple metric (e.g.
euclidean distance) represents semantic similarity.

$f_{\theta}$ is typically a CNN which outputs a fixed dense representation of the image.

--

For other problems (sound, NLP), different networks may be used (RNNs, ...) which
output a fixed dimension vector representing the data.

--

Alternatively we compute a similarity function instead of a distance:

$cosine(f\_{\theta}(x\_1), f\_{\theta}(x\_2))$

--

Training = representation leanrnig

---
## Siamese networks

.center[
          <img src="images/siamese.svg" style="width: 530px;" />
]

.footnote.small[
Chopra, Sumit, Raia Hadsell, and Yann LeCun. "Learning a similarity metric discriminatively, with application to face verification." CVPR 2005.
]

--
Two absolutely identical networks. They output a fixed dimension, e.g. $128$

---
## Training

- Dataset
- build positive and negative pairs
- Contrastive loss
- results

.footnote.small[
Taigman et. al., 2014. DeepFace closing the gap to human level performance
]
---
## Triplet Loss

A triplet: $(x^a, x^{+}, x^{-})$
- an anchor image
- positive image (same person as anchor)
- negative image (different person as enchor)

We compute f_{\theta} of each of these 3 images

--
We want that:

$||f(x^a) - f(x^{+})||_2 \leq ||f(x^a) - f(x^{-})||_2$

--

$||f(x^a) - f(x^{+})||_2 - ||f(x^a) - f(x^{-})||_2 \leq 0$

--

$||f(x^a) - f(x^{+})||_2 - ||f(x^a) - f(x^{-})||_2 \leq - \alpha$

$\alpha$ margin typically small positive number $(0.2, 0.5, \cdot)$
???

Although we did not a do direct comparison to other
losses, e.g. the one using pairs of positives and negatives, as
used in [14] Eq. (2), we believe that the triplet loss is more
suitable for face verification. The motivation is that the loss
from [14] encourages all faces of one identity to be projected
Z onto a single point in the embedding space. The
triplet loss, however, tries to enforce a margin between each
pair of faces from one person to all other faces. This allows
the faces for one identity to live on a manifold, while
still enforcing the distance and thus discriminability to other
identities.

---
##Triplet Loss

$l(x^a, x^{+}, x^{-}) =$

$max(||f(x^a) - f(x^{+})||_2 -
                           ||f(x^a) - f(x^{-})||_2 + \alpha, 0)$

--

###Training

- Sample a minibatch of triplets $\{(x^a, x^{+}, x^{-})_i\}$
- Compute $\sum\_i l((x^a, x^{+}, x^{-})\_i)$
- Propagate gradients to update the $f_{\theta}$


.footnote.small[
Schroff et al.,2015, FaceNet: A unified embedding for face recognition and clustering
]

---
## Hard negative sampling

If anchor, positive and negative are chosen randomly, gradient in one batch quickly go to $0$

--

Schedule and chose triplets that are hard

What we want to do is choose triplets that are hard to train on.

???
Commercial recognition systems are trained on a large datasets like 10/100 million images.
There are a lot of pretrained models and parameters online for face recognition.
Face Verification and Binary Classification
Triplet loss is one way to learn the parameters of a conv net for face recognition there's another way to learn these parameters as a straight binary classification problem.
Learning the similarity function another way:

The final layer is a sigmoid layer.
Y' = wi * Sigmoid ( f(x(i)) - f(x(j)) ) + b where the subtraction is the Manhattan distance between f(x(i)) and f(x(j))
Some other similarities can be Euclidean and Ki square similarity.
The NN here is Siamese means the top and bottom convs has the same parameters.
The paper for this work: [Taigman et. al., 2014. DeepFace closing the gap to human level performance]
A good performance/deployment trick:
Pre-compute all the images that you are using as a comparison to the vector f(x(j))
When a new image that needs to be compared, get its vector f(x(i)) then put it with all the pre computed vectors and pass it to the sigmoid function.
This version works quite as well as the triplet loss function.
Available implementations for face recognition using deep learning includes:
Openface
FaceNet
DeepFace

---
class: middle, center

# Lab 1: Room C48-C49 in 15min!

    </textarea>
    <style TYPE="text/css">
      code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}
    </style>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
      tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry
      }
      });
      MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i = 0; i < all.length; i += 1) {
		     all[i].SourceElement().parentNode.className += ' has-jax';
		     }
		     });
		     </script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script src="../remark.min.js" type="text/javascript">
    </script>
    <script type="text/javascript">
      var slideshow = remark.create({
        highlightStyle: 'github',
        highlightSpans: true,
        highlightLines: true
      });
    </script>
  </body>
</html>
